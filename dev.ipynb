{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('resources/input.txt', 'r', encoding='utf-8') as f: text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of text:  1115394\n"
     ]
    }
   ],
   "source": [
    "print(\"length of text: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at the first 1000 chracaters\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size:  65\n",
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "# here are all the unique character in the dataset\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "print(\"vocab size: \", vocab_size)\n",
    "print(''.join(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 43, 50, 50, 53]\n",
      "hello\n"
     ]
    }
   ],
   "source": [
    "# mapping characters to integers\n",
    "stoi = {ch:i for i,ch in enumerate(chars)}\n",
    "itos = {i:ch for i,ch in enumerate(chars)}\n",
    "\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda li: ''.join([itos[i] for i in li])\n",
    "\n",
    "print(encode('hello'))\n",
    "print(decode([46, 43, 50, 50, 53]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, '\\n')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stoi['\\n'], itos[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394])\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
     ]
    }
   ],
   "source": [
    "# let's encode the whole text\n",
    "import torch\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "print(data.shape)\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test split\n",
    "split = int(0.9*len(data))\n",
    "train_data = data[:split]\n",
    "val_data = data[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When input is tensor([18]) the target: 47\n",
      "When input is tensor([18, 47]) the target: 56\n",
      "When input is tensor([18, 47, 56]) the target: 57\n",
      "When input is tensor([18, 47, 56, 57]) the target: 58\n",
      "When input is tensor([18, 47, 56, 57, 58]) the target: 1\n",
      "When input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\n",
      "When input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n",
      "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n"
     ]
    }
   ],
   "source": [
    "# the context length: # characters the model will see at a time\n",
    "block_size = 8\n",
    "\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"When input is {context} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Indices (ix): tensor([ 71565, 224657, 932192, 557350])\n",
      "\n",
      "Shapes: Input torch.Size([4, 8]) Lable torch.Size([4, 8])\n",
      "\n",
      "Input: tensor([[ 1, 60, 39, 47, 50,  1, 63, 53],\n",
      "        [46, 43, 39, 60, 43, 52,  1, 44],\n",
      "        [ 1, 46, 43, 56, 43,  1, 63, 53],\n",
      "        [61, 47, 50, 50,  1, 57, 39, 63]])\n",
      "\n",
      "Label: tensor([[60, 39, 47, 50,  1, 63, 53, 59],\n",
      "        [43, 39, 60, 43, 52,  1, 44, 53],\n",
      "        [46, 43, 56, 43,  1, 63, 53, 59],\n",
      "        [47, 50, 50,  1, 57, 39, 63,  0]])\n",
      "\n",
      "When input is [1] the target: 60\n",
      "When input is [1, 60] the target: 39\n",
      "When input is [1, 60, 39] the target: 47\n",
      "When input is [1, 60, 39, 47] the target: 50\n",
      "When input is [1, 60, 39, 47, 50] the target: 1\n",
      "When input is [1, 60, 39, 47, 50, 1] the target: 63\n",
      "When input is [1, 60, 39, 47, 50, 1, 63] the target: 53\n",
      "When input is [1, 60, 39, 47, 50, 1, 63, 53] the target: 59\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4\n",
    "block_size = 8\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(high=len(data) - block_size, size=(batch_size,)) # 4 random integers betweeen 0 to len(data)-batch_size\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])                 # next 8 characters from each random integer\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])             # next 8 characters from first character in x\n",
    "    return x,y, ix                                                      # returning ix for debugging purpose\n",
    "\n",
    "xb, yb, ix = get_batch('train')\n",
    "\n",
    "print(f\"Random Indices (ix): {ix}\\n\")\n",
    "print(f\"Shapes: Input {xb.shape} Lable {yb.shape}\\n\")\n",
    "print(f\"Input: {xb}\\n\")\n",
    "print(f\"Label: {yb}\", end='\\n\\n')\n",
    "\n",
    "for b in range(batch_size):\n",
    "    for t in range(block_size):\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"When input is {context.tolist()} the target: {target}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1, 60, 39, 47, 50,  1, 63, 53],\n",
      "        [46, 43, 39, 60, 43, 52,  1, 44],\n",
      "        [ 1, 46, 43, 56, 43,  1, 63, 53],\n",
      "        [61, 47, 50, 50,  1, 57, 39, 63]])\n"
     ]
    }
   ],
   "source": [
    "print(xb) # our input to the transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[60, 39, 47, 50,  1, 63, 53, 59],\n",
       "        [43, 39, 60, 43, 52,  1, 44, 53],\n",
       "        [46, 43, 56, 43,  1, 63, 53, 59],\n",
       "        [47, 50, 50,  1, 57, 39, 63,  0]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yb # our target for the transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Simple Langage Model\n",
    " - Only has an embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.5629, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "l-QYjt'CL?jLDuQcLzy'RIo;'KdhpV\n",
      "vLixa,nswYZwLEPS'ptIZqOZJ$CA$zy-QTkeMk x.gQSFCLg!iW3fO!3DGXAqTsq3pdgq\n"
     ]
    }
   ],
   "source": [
    "# starting with a bigram model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size) # representing each token as a vector of dim=65 (vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx and targets are of shape (batch_size, block_size) or (B, T)\n",
    "        logits = self.token_embedding_table(idx) # shape: (batch_size, block_size, vocab_size) or (B, T, C)\n",
    "        if targets is None: loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "    \n",
    "m = BigramLanguageModel(vocab_size) # vocab_size = 65\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "\n",
    "print(loss)\n",
    "\n",
    "idx = torch.zeros((1,1), dtype=torch.long)\n",
    "generated = decode(m.generate(idx, max_new_tokens=100)[0].tolist())\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(m.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 999, Loss: 3.667173385620117\n"
     ]
    }
   ],
   "source": [
    "batch_size  = 32\n",
    "\n",
    "for steps in range(1000):\n",
    "    # sample a batch of data\n",
    "    xb, yb, _ = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(f\"Step: {steps}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The meaning of life ishLLkoexMyas:Iocindad.e-NNSqYPso&bFho&$;BQ$dZTMf'fKlf;DRPm'W,esPHyXAzCA$;GunqCEy&Oy;ZxjKVhmrdhxCAbTSp-Rue.HQNNbxF&kwst-s\n",
      "OMckHNENUZEzlda$BPPtIhe!,epdheaRns:\n",
      "AqW3E,DXU,NENT n b.u.xIYV&j'nnl;s,ngtoOm ixPLenrXElrPjIU-T'St3PJ\n",
      "cra3bLGhT ALM-veAYkpr ,erPVhJQNV\n",
      "P?WN3b?oYxpig;ENTy3q&j.mes; iZ,w..w&yEK\n",
      "Ona$IyYWi.OU ay;,weP?AqV-XAPig;\n",
      "OMGBI3Dor,EL.xy\n",
      "OZ r!Nxx-shz!q pZrAQll'vPkntezN\n",
      "BPy3motRMqFhoPpCbenYxubek,-Z:qddF\n",
      "NIgmoP\n",
      "TEhoXhn:B?ZEMv?nlnsFprzyAgwNodd  bGP EAlprPeQnqDGhdDUP.JIGNaVDwfIxx.c XhoCb&DivRS&fuF\n"
     ]
    }
   ],
   "source": [
    "context = torch.tensor(encode(\"The meaning of life is\")).reshape(1, -1)\n",
    "# idx = torch.zeros((1,1), dtype=torch.long)\n",
    "generated = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The mathematical trick in self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# consider the following example\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "B,T,C = 4,8,2 # batch size, timesteps, channels\n",
    "x = torch.randn(B,T,C) # random input\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we want is, some communication across timestep dimention. so the nth token should know how it's related to all the previous tokens.\n",
    "\n",
    "- example (using words for simplicity): \n",
    "    - timesteps = [\"the\", \"meaning\", \"of\", \"life\", \"is\"]\n",
    "    - this will get vecotrized to let's say [0, 1, 2, 3, 4]\n",
    "    - we want each element from the vector to be something related to all the previous elements (and not the future elements because that would be cheating in case of predicting next token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simplest thing we can do is make the nth token to be the average of the first n tokens\n",
    "\n",
    "# we want x[b, t] = mean_{i<=t} x[b, i]\n",
    "torch.manual_seed(42)\n",
    "\n",
    "xbow = torch.zeros((B,T,C)) # x_bag_of_words\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b,:t+1] # (t, C)\n",
    "        xbow[b,t] = torch.mean(xprev, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "b tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "c tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "# an efficient way to do this is... here's an example  \n",
    "torch.manual_seed(42)\n",
    "\n",
    "a = torch.tril(torch.ones(3,3)) # lower triangular matrix\n",
    "a = a / a.sum(dim=1, keepdim=True)\n",
    "b = torch.randint(0, 10, (3,2)).float() \n",
    "c = a @ b\n",
    "\n",
    "print(\"a\", a)\n",
    "print(\"b\", b)\n",
    "print(\"c\", c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# version 2\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "wei = torch.tril(torch.ones(T,T))\n",
    "wei = wei / wei.sum(dim=1, keepdim=True)\n",
    "xbow2 = wei @ x # (T, T) @ (B, T, C) -> (B, T, C)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xbow[0], xbow2[0] # pretty close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# version 3: using softmax\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril==0, float('-inf')) # setting the upper (zeros) triangle to -inf\n",
    "wei = F.softmax(wei, dim=-1) # in each row, zeros will be replaced with probabilities, -inf will be zeros\n",
    "xbow3 = wei @ x\n",
    "\n",
    "xbow[0], xbow3[0] # pretty close"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "look at how the future elements are not used in the calculation :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tril) # a mask to zero out the future time steps\n",
    "print(wei) # scores for each time step\n",
    "\n",
    "# finally, xbow3 is the weighted sum of all time steps\n",
    "print(xbow3[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SELF-ATTENTION** (from 31b1 video on transformers)\n",
    "\n",
    "![](./resources/self-attention.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# version 4: self-attention\n",
    "torch.manual_seed(42)\n",
    "\n",
    "B,T,C = 4,8,32 # batch_size, timesteps, channels\n",
    "x = torch.randn(B,T,C)  \n",
    "\n",
    "# a sinlge self-attention head\n",
    "head_size = 16 # hyperparameter: output dim of the self-attention head\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "k = key(x) # (B, T, head_size)\n",
    "q = query(x) # (B, T, head_size)\n",
    "\n",
    "wei = q @ k.transpose(1, 2) # (B, T, head_size) @ (B, head_size, T) -> (B, T, T)\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "wei = wei.masked_fill(tril==0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "v = value(x) # (B, T, head_size)\n",
    "out = wei @ v\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- Attention is a **communication mechanism**. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
    "\n",
    "- There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n",
    "\n",
    "- Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n",
    "\n",
    "- In an \"encoder\" attention block just deletes the single line (line no. 20) that does masking with `tril`, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n",
    "\n",
    "- \"self-attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n",
    "\n",
    "- \"Scaled\" attention additional divides `wei` by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Only difference between LayerNorm1d and Batchnorm is the dimension we normalize.\n",
    "# In Batchnorm we normalize the columns of the input, while in LayerNorm1d we normalize the rows of the input.\n",
    "# We also don't need to maintain training and evaluation buffers in LayerNorm1d, since we normalize the rows of the input.\n",
    "\n",
    "class LayerNorm1d: # (used to be BatchNorm1d)\n",
    "\n",
    "  def __init__(self, dim, eps=1e-5):\n",
    "    self.eps = eps\n",
    "    self.gamma = torch.ones(dim)\n",
    "    self.beta = torch.zeros(dim)\n",
    "\n",
    "  def __call__(self, x):\n",
    "    # calculate the forward pass\n",
    "    xmean = x.mean(1, keepdim=True) # here's the difference between LayerNorm1d and Batchnorm\n",
    "    xvar = x.var(1, keepdim=True)   # normalizing the rows instead of columns\n",
    "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
    "    self.out = self.gamma * xhat + self.beta\n",
    "    return self.out\n",
    "\n",
    "  def parameters(self):\n",
    "    return [self.gamma, self.beta]\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "module = LayerNorm1d(100)\n",
    "x = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\n",
    "x = module(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1469), tensor(0.8803))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x is not normalized to column-wise \n",
    "x[:,0].mean(), x[:,0].std() # mean,std of one feature across all batch inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-9.5367e-09), tensor(1.0000))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x is normalized to row-wise\n",
    "x[0,:].mean(), x[0,:].std() # mean,std of a single input from the batch, of its features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
